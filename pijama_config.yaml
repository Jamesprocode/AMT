# ── Data paths ────────────────────────────────────────────────────────────────
data_dir: PiJAMa/midi_kong      # folder to read .midi files from (midi or midi_kong)
output_dir: data/pijama-aug10        # where train.txt / valid.txt are written

# ── Preprocessing ─────────────────────────────────────────────────────────────
augment_factor: 10              # 1 = autoregressive only
                                # 10 = full anticipation augmentation (spans + random)
seed: 42
workers: 8                      # parallel CPU workers for preprocessing (default: all cores)

# ── Model ─────────────────────────────────────────────────────────────────────
model: /home/hice1/jwang3180/scratch/AMT/model/music-medium-800k   # HF model ID or local checkpoint path
lora: false                               # true = LoRA (~8GB VRAM), false = full finetune (~24GB)
checkpoint_dir: /home/hice1/jwang3180/scratch/AMT/checkpoints/pijama-full-aug10      # where to save checkpoints
resume_from_checkpoint: null            # set to a checkpoint path to resume, e.g. checkpoints/pijama/checkpoint-1000

# ── Training hyperparameters ──────────────────────────────────────────────────
epochs: 10
batch_size: 4                 # per-device; H100 80GB can handle this easily
grad_accum: 8                 # effective batch = batch_size * grad_accum (= 32)
lr: 5.0e-6                    # conservative for full finetune, avoid catastrophic forgetting

# ── Checkpointing ─────────────────────────────────────────────────────────────
save_steps: 1000                # save a checkpoint every N steps
save_total_limit: 5             # keep only the N most recent checkpoints (+ best)
eval_steps: 200                 # evaluate every N steps (finer-grained loss curve)

# ── Sampling / logging ────────────────────────────────────────────────────────
sample_every_steps: 500         # generate a MIDI audio sample every N steps and log to W&B
sample_length: 10               # length of generated sample in seconds

# ── W&B ───────────────────────────────────────────────────────────────────────
wandb_project: amt-pijama
wandb_run: amt-pijama-full-10ep-aug10

